{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for a data warehouse\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This is the capstone project of the udacity nanodegree for data engineering. \n",
    "The aim of the project is to apply learned skills during the course. \n",
    "\n",
    "This project will show how to load and transform data from four different data sources, load the data in spark apply quality checks and store the data into a star schema so that it can be used for BI apps and ad hoc analysis. \n",
    "\n",
    "Specific analysis of the number of immigrants in relation to weather and locations and time can be executed with the created data model. Examples can be found in `capstone_sample_analysis.ipynb`\n",
    "\n",
    "The project follows the follow steps:\n",
    "* _Step 1_: Scope the Project and Gather Data\n",
    "* _Step 2_: Explore and Assess the Data\n",
    "* _Step 3_: Define the Data Model\n",
    "* _Step 4_: Run ETL to Model the Data\n",
    "* _Step 5_: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "As one of the acceptance criteria for this project is to handle at least 1 million rows and two different data sources and file formats, we will use the data sources are provided by Udacity. \n",
    "The scope of this project is to create a star schema source-of-truth database so that it can be used for BI apps and ad hoc analysis.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "The main dataset includes data about \n",
    "- _Immigration in the United States of America_ from [here](https://www.trade.gov/national-travel-and-tourism-office).\n",
    "\n",
    "Supplementary datasets provided are:\n",
    "\n",
    "- U.S. city _demographics_ from [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    "\n",
    "- _Temperature_ data from [here](https://www.trade.gov/national-travel-and-tourism-office)\n",
    "\n",
    "- Data on _airport codes_ from [here](https://datahub.io/core/airport-codes#data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "The data shall be used for ad hoc queries and BI Apps. Therefore the data shall be represented in a star schema.\n",
    "The advantage is, that it is easy to query an easy to understand.\n",
    "\n",
    "\n",
    "![star schema](capstone_schema.png)\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "In order to pipe the data into the chosen data model, we will use Spark. This has the advantage that we can do the transformations in memory, before writing the data to the tables.\n",
    "1. Load only the needed columns from the immigrant files into a spark dataframe. \n",
    "2. Transform arrival and departure date to timestamps\n",
    "3. Create the fact_immigration table from the loaded immigrant files and write to parquet files.\n",
    "4. Create the dim_immigrant_person table from the loaded immigrant files and write to parquet files.\n",
    "5. Create the dim_time table from the transformed timestamps\n",
    "6. Load weather data, filter the weather data by country, as we only need data from the United States from 1960 on and save them as dim_weather\n",
    "7. Load the city data and write to parquet files as dim_city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n",
    "The code can be found in `etl.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"Capstone Project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load the needed immigration data from parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import etl_capstone\n",
    "\n",
    "df_immigration = etl_capstone.get_df_immigration(spark)\n",
    "df_immigration.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2: Create the fact table and write it to a parquet table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl_capstone.create_fact_immigrant(df_immigration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step3: Create date dimension table and write it to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "etl_capstone.create_dim_date(df_immigration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step4: Create city dimension table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step5: Create weather dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl_capstone.create_dim_weather(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Create immigration person dimenston table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl_capstone.create_dim_immigrant(df_immigration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks \n",
    "\n",
    "- We are using  _schemas_, when reading the data, with the schema we can assure that the data type is correct. See `schemas.py`\n",
    "- We are using _dropDuplicates()_ to get rid of duplicate data.\n",
    "- We are count the number of rows and columns for the output tables. See `utils.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "#### FACT IMMIGRATION\n",
    "\n",
    "| column name| description | type | data source | \n",
    "| --- | --- | --- | --- |\n",
    "|cicid | unique number for an immigrant | integer, not nullable | sas_data  immigration |\n",
    "|arrival_ts | timestamp  of the arrival date | timestamp | sas_data  immigration: transformed from field \"arrdata\": SAS numeric |\n",
    "|departure_ts | timestamp  of the arrival date | timestamp | sas_data  immigration: transformed from field \"depdate\": SAS numeric |\n",
    "|i94cit | 3 digit code of origin city | short, not nullable | sas_data  immigration |\n",
    "|i94res | 3 digit from the country one has travelled | short | sas_data  immigration |\n",
    "|i94port | 3 char code of origin city in USA| string | sas_data  immigration |\n",
    "|fltno | flight number of airline that arrived in us | string | sas_data  immigration |\n",
    "\n",
    "#### DIM TIME\n",
    "\n",
    "Datasource: all timestamps are taken frome the arrival and departure date of the fact_immigration table\n",
    "\n",
    "| column name| description | type |\n",
    "| --- | --- | --- | \n",
    "| ts | unix timestamp, not nullable | ts |\n",
    "| date | date | string |\n",
    "| year | year | integer |\n",
    "| month | month | integer |\n",
    "| weekday | weekday | integer |\n",
    "| day | day | integer |\n",
    "| hour | hour | integer |\n",
    "\n",
    "#### DIM IMMIGRANT\n",
    "Data Source:  sas_data  immigration\n",
    "\n",
    "| column name| description | type |\n",
    "| --- | --- | --- | \n",
    "|cicid|  unique number for the immigrants| int |\n",
    "|biryear| year of birth| int |\n",
    "|gender| gender of immigrant| string |\n",
    "|i94visa| Visa codes collapsed into three categories, 1 = Business 2 = Pleasure 3 = Student |int |\n",
    "\n",
    "\n",
    "#### DIM CITY\n",
    "\n",
    "Datasource: https://public.opendatasoft.com/explore/dataset/us-cities-demographics\n",
    "\n",
    "| column name| description | type | \n",
    "| --- | --- | --- |\n",
    "|city_code| code for the city| string |\n",
    "|city_name| name of the city| string |\n",
    "|state | state | string |\n",
    "|male_population | number of male population| int |\n",
    "|male_population | number of female population| int |\n",
    "|total_population | number of total population| int |\n",
    "|foreign_born | number of foreign born | int |\n",
    "|average_household | average number of people in a household | double |\n",
    "| state_code | code of the state | int |\n",
    "\n",
    "\n",
    "#### DIM TEMPERATURE\n",
    "\n",
    "| column name| description | type |\n",
    "| --- | --- | --- | \n",
    "|dt|date|date|\n",
    "|AverageTemperature| <- |float|\n",
    "|AverageTemperatureUncertainty|<- |string|\n",
    "|city_code| code for the city| string |\n",
    "|City|<- | string|\n",
    "|Country|<- | string|\n",
    "|year| <- | integer|\n",
    "|month| <- |integer|\n",
    "|Langitude| <- |integer|\n",
    "|Latidue| <- |integer|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "1. _Clearly state the rationale for the choice of tools and technologies for the project._\n",
    "    We choose Apache Spark, because\n",
    "    - we are able to read in only the columns we need from parquet files\n",
    "    - we are able to do data cleaning and transformation either with dataframes or sql syntax.\n",
    "    - it provides an easy to use API\n",
    "    - it can handle a lot of different file formats \n",
    "    - it can handle big amounts of data  <br/>\n",
    "  \n",
    "\n",
    "2. _Propose how often the data should be updated and why._\n",
    "    - It depends on the amount of data and how often the data is updated. If the data is updated every week, it makes sense to run the pipeline every week. <br/>\n",
    "\n",
    "\n",
    "3. _Write a description of how you would approach the problem differently under the following scenarios:_  <br/>\n",
    "\n",
    " 3a. _The data was increased by 100x._<br/>\n",
    "    - The bigger the data gets, the more computing power is helpful to process the data. Adding more nodes to our cluster could be a way of dealing with a bigger amount of data.\n",
    "\n",
    "\n",
    " 3b. _The data populates a dashboard that must be updated on a daily basis by 7am every day._<br/>\n",
    "    - For scheduled pipelines a tool like [Airflow](https://airflow.apache.org/docs/) can be used. It has the advantage, that it provides a web view, so that also non programmers can check wether a pipeline ran successfully or not.\n",
    "\n",
    "\n",
    " 3c. _The database needed to be accessed by 100+ people._<br/>\n",
    "    - There are different cloud solutions availabe for this scenario. We could use [Databricks](https://databricks.com/) it can handle a lot of simultaneasly connections.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
