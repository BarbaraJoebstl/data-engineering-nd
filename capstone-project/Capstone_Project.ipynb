{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for a data warehouse\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This is the capstone project of the udacity nanodegree for data engineering. \n",
    "The aim of the project is to apply learned skills during the course. \n",
    "\n",
    "This project will show how to load and transform data from four different data sources, load the data in spark apply quality checks and store the data into a star schema so that it can be used for BI apps and ad hoc analysis. \n",
    "\n",
    "Specific analysis of the number of immigrants in relation to weather and locations and time can be executed with the created data model. An example can be found in `capstone_sample_analysis.ipynb`\n",
    "\n",
    "The project follows the follow steps:\n",
    "* _Step 1_: Scope the Project and Gather Data\n",
    "* _Step 2_: Explore and Assess the Data\n",
    "* _Step 3_: Define the Data Model\n",
    "* _Step 4_: Run ETL to Model the Data\n",
    "* _Step 5_: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "As one of the acceptance criteria for this project is to handle at least 1 million rows and two different data sources and file formats, we will use the data sources are provided by Udacity. \n",
    "The scope of this project is to create a star schema source-of-truth database so that it can be used for BI apps and ad hoc analysis.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "The main dataset includes data about \n",
    "- _Immigration in the United States of America_ from [here](https://www.trade.gov/national-travel-and-tourism-office).\n",
    "\n",
    "Supplementary datasets provided are:\n",
    "\n",
    "- U.S. city _demographics_ from [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    "\n",
    "- _Temperature_ data from [here](https://www.trade.gov/national-travel-and-tourism-office)\n",
    "\n",
    "- Data on _airport codes_ from [here](https://datahub.io/core/airport-codes#data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "The data shall be used for ad hoc queries and BI Apps. Therefore the data shall be represented in a star schema.\n",
    "The advantage is, that it is easy to query an easy to understand.\n",
    "\n",
    "\n",
    "![star schema](capstone_schema.png)\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "In order to pipe the data into the chosen data model, we will use Spark. This has the advantage that we can do the transformations in memory, before writing the data to the tables.\n",
    "1. Load only the needed columns from the immigrant files into a spark dataframe. \n",
    "2. Transform arrival and departure date to timestamps\n",
    "3. Create the fact_immigration table from the loaded immigrant files and write to parquet files.\n",
    "4. Create the dim_immigrant_person table from the loaded immigrant files and write to parquet files.\n",
    "5. Create the dim_time table from the transformed timestamps\n",
    "6. Load weather data, filter the weather data by country, as we only need data from the United States from 1960 on and save them as dim_weather\n",
    "7. Load the city data and write to parquet files as dim_city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n",
    "The code can be found in `etl.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"Capstone Project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load the needed immigration data from parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- arrival_ts: string (nullable = true)\n",
      " |-- departure_ts: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import etl_capstone\n",
    "\n",
    "df_immigration = etl_capstone.get_df_immigration(spark)\n",
    "df_immigration.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2: Create the fact table and write it to a parquet table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Current DF contains 219268 rows\n",
      "Info: 7 columnns for current DF.\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- arrival_ts: string (nullable = true)\n",
      " |-- departure_ts: string (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Immigration fact table written to: tables/fact_immigration\n"
     ]
    }
   ],
   "source": [
    "\n",
    "etl_capstone.create_fact_immigrant(df_immigration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step3: Create date dimension table and write it to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Current DF contains 438536 rows\n",
      "Info: 8 columnns for current DF.\n",
      "Success: date dimension data validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:>                                                       (0 + 8) / 400]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-01 14:57:32 WARN  MemoryManager:115 - Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "2022-03-01 14:57:32 WARN  MemoryManager:115 - Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "2022-03-01 14:57:32 WARN  MemoryManager:115 - Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: date dimension table written to: tables/dim_time\n"
     ]
    }
   ],
   "source": [
    "\n",
    "etl_capstone.create_dim_date(df_immigration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step4: Create city dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city_name: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male_population: integer (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- foreign_born: integer (nullable = true)\n",
      " |-- average_householdsize: double (nullable = true)\n",
      " |-- state_code: integer (nullable = true)\n",
      " |-- city_code: string (nullable = true)\n",
      "\n",
      "Info: Current DF contains 2891 rows\n",
      "Error: There are 9 instead of 8 for the current Dataframe\n",
      "Success: city dimension data validation\n",
      "2022-03-01 14:58:31 WARN  CSVDataSource:66 - Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 12, schema size: 8\n",
      "CSV file: file:///Users/joebsbar/Documents/GitBarbara/data-engineering-nd/capstone-project/data/us-cities-demographics.csv\n",
      "Info: city dimension table written to: tables/dim_city\n"
     ]
    }
   ],
   "source": [
    "etl_capstone.create_dim_city(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step5: Create weather dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Current DF contains 134925 rows\n",
      "Error: There are 11 instead of 7 for the current Dataframe\n",
      "Success: temparature dimension data validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: temperature dimension table written to: tables/dim_temperature\n"
     ]
    }
   ],
   "source": [
    "etl_capstone.create_dim_weather(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Create immigration person dimenston table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Current DF contains 219268 rows\n",
      "Info: 4 columnns for current DF.\n",
      "Success: immigrant person dimension data validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-01 14:59:13 WARN  MemoryManager:115 - Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: immigrant dimension table written to: tables/dim_immigrant_person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 49669)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/joebsbar/opt/anaconda3/envs/dend/lib/python3.7/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/joebsbar/opt/anaconda3/envs/dend/lib/python3.7/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/joebsbar/opt/anaconda3/envs/dend/lib/python3.7/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/joebsbar/opt/anaconda3/envs/dend/lib/python3.7/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/joebsbar/opt/anaconda3/envs/dend/lib/python3.7/site-packages/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/joebsbar/opt/anaconda3/envs/dend/lib/python3.7/site-packages/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/Users/joebsbar/opt/anaconda3/envs/dend/lib/python3.7/site-packages/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/Users/joebsbar/opt/anaconda3/envs/dend/lib/python3.7/site-packages/pyspark/serializers.py\", line 714, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "etl_capstone.create_dim_immigrant(df_immigration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks \n",
    "\n",
    "- We are using  _schemas_, when reading the data, with the schema we can assure that the data type is correct. See `schemas.py`\n",
    "- We are using _dropDuplicates()_ to get rid of duplicate data.\n",
    "- We are count the number of rows and columns for the output tables. See `utils.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "#### FACT IMMIGRATION\n",
    "\n",
    "| column name| description | type | data source | \n",
    "| --- | --- | --- | --- |\n",
    "|cicid | unique number for an immigrant | integer, not nullable | sas_data  immigration |\n",
    "|arrival_ts | timestamp  of the arrival date | timestamp | sas_data  immigration: transformed from field \"arrdata\": SAS numeric |\n",
    "|departure_ts | timestamp  of the arrival date | timestamp | sas_data  immigration: transformed from field \"depdate\": SAS numeric |\n",
    "|i94cit | 3 digit code of origin city | short, not nullable | sas_data  immigration |\n",
    "|i94res | 3 digit from the country one has travelled | short | sas_data  immigration |\n",
    "|i94port | 3 char code of origin city in USA| string | sas_data  immigration |\n",
    "|fltno | flight number of airline that arrived in us | string | sas_data  immigration |\n",
    "\n",
    "#### DIM TIME\n",
    "\n",
    "Datasource: all timestamps are taken frome the arrival and departure date of the fact_immigration table\n",
    "\n",
    "| column name| description | type |\n",
    "| --- | --- | --- | \n",
    "| ts | unix timestamp, not nullable | ts |\n",
    "| date | date | string |\n",
    "| year | year | integer |\n",
    "| month | month | integer |\n",
    "| weekday | weekday | integer |\n",
    "| day | day | integer |\n",
    "| hour | hour | integer |\n",
    "\n",
    "#### DIM IMMIGRANT\n",
    "Data Source:  sas_data  immigration\n",
    "\n",
    "| column name| description | type |\n",
    "| --- | --- | --- | \n",
    "|cicid|  unique number for the immigrants| int |\n",
    "|biryear| year of birth| int |\n",
    "|gender| gender of immigrant| string |\n",
    "|i94visa| Visa codes collapsed into three categories, 1 = Business 2 = Pleasure 3 = Student |int |\n",
    "\n",
    "\n",
    "#### DIM CITY\n",
    "\n",
    "Datasource: https://public.opendatasoft.com/explore/dataset/us-cities-demographics\n",
    "\n",
    "| column name| description | type | \n",
    "| --- | --- | --- |\n",
    "|city_code| code for the city| string |\n",
    "|city_name| name of the city| string |\n",
    "|state | state | string |\n",
    "|male_population | number of male population| int |\n",
    "|male_population | number of female population| int |\n",
    "|total_population | number of total population| int |\n",
    "|foreign_born | number of foreign born | int |\n",
    "|average_household | average number of people in a household | double |\n",
    "| state_code | code of the state | int |\n",
    "\n",
    "\n",
    "#### DIM TEMPERATURE\n",
    "\n",
    "| column name| description | type |\n",
    "| --- | --- | --- | \n",
    "|dt|date|date|\n",
    "|AverageTemperature| <- |float|\n",
    "|AverageTemperatureUncertainty|<- |string|\n",
    "|city_code| code for the city| string |\n",
    "|City|<- | string|\n",
    "|Country|<- | string|\n",
    "|year| <- | integer|\n",
    "|month| <- |integer|\n",
    "|Langitude| <- |integer|\n",
    "|Latidue| <- |integer|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "1. _Clearly state the rationale for the choice of tools and technologies for the project._\n",
    "    We choose Apache Spark, because\n",
    "    - we are able to read in only the columns we need from parquet files\n",
    "    - we are able to do data cleaning and transformation either with dataframes or sql syntax.\n",
    "    - it provides an easy to use API\n",
    "    - it can handle a lot of different file formats \n",
    "    - it can handle big amounts of data  <br/>\n",
    "  \n",
    "\n",
    "2. _Propose how often the data should be updated and why._\n",
    "    - It depends on the amount of data and how often the data is updated. If the data is updated every week, it makes sense to run the pipeline every week. <br/>\n",
    "\n",
    "\n",
    "3. _Write a description of how you would approach the problem differently under the following scenarios:_  <br/>\n",
    "\n",
    " 3a. _The data was increased by 100x._<br/>\n",
    "    - The bigger the data gets, the more computing power is helpful to process the data. Adding more nodes to our cluster could be a way of dealing with a bigger amount of data.\n",
    "\n",
    "\n",
    " 3b. _The data populates a dashboard that must be updated on a daily basis by 7am every day._<br/>\n",
    "    - For scheduled pipelines a tool like [Airflow](https://airflow.apache.org/docs/) can be used. It has the advantage, that it provides a web view, so that also non programmers can check wether a pipeline ran successfully or not.\n",
    "\n",
    "\n",
    " 3c. _The database needed to be accessed by 100+ people._<br/>\n",
    "    - There are different cloud solutions availabe for this scenario. We could use [Databricks](https://databricks.com/) it can handle a lot of simultaneasly connections.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
