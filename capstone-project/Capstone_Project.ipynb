{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for a data warehouse\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This is the capstone project of the udacity nanodegree for data engineering. \n",
    "The aim of the project is to apply learned skills during the course. This project will show how to load and transform data from four different data sources, load the data in spark apply quality checks and store the data into a star schema so that it can be used for BI apps and ad hoc analysis.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* _Step 1_: Scope the Project and Gather Data\n",
    "* _Step 2_: Explore and Assess the Data\n",
    "* _Step 3_: Define the Data Model\n",
    "* _Step 4_: Run ETL to Model the Data\n",
    "* _Step 5_: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "As one of the acceptance criteria for this project is to handle at least 1 million rows and two different data sources and file formats, we will use the data sources are provided by Udacity. \n",
    "The scope of this project is to create a star schema source-of-truth database so that it can be used for BI apps and ad hoc analysis.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "The main dataset includes data about \n",
    "- _Immigration in the United States of America_ from [here](https://www.trade.gov/national-travel-and-tourism-office).\n",
    "\n",
    "Supplementary datasets provided are:\n",
    "\n",
    "- U.S. city _demographics_ from [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    "\n",
    "- _Temperature_ data from [here](https://www.trade.gov/national-travel-and-tourism-office)\n",
    "\n",
    "- Data on _airport codes_ from [here](https://datahub.io/core/airport-codes#data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, desc, asc, to_date\n",
    "from pyspark.sql.types import StructType, StructField as Fld, StringType as Str, IntegerType as Int, ShortType as Short, DoubleType as Dbl, ByteType as Bt\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Explore and Asses the Data\n",
    "The exploration of the used datasets including descriptions of the above mentioned data can be found in `data/exploration.ipynb`.\n",
    "\n",
    "#### Important findings of the exploration\n",
    "\n",
    "##### Immigrant data:\n",
    " -- dates are provided in SAS timestamps and need to be transformed\n",
    "\n",
    " -- there are many columns that have a lot of null values or are marked with CIC does not use\", hence can be skipped. As the immigrant data is provided in parquet files, we can only read in the columns we need for this project.\n",
    "\n",
    " -- We might need helper tables to provide data for the\n",
    "\n",
    " --- visa codes: 1 = Business 2 = Pleasure 3 = Student \n",
    " \n",
    " --- travel code:  1 = 'Air' 2 = 'Sea' 3 = 'Land' 9 = 'Not \n",
    "\n",
    "##### City demographics data:\n",
    "- the \"count\" column can be skipped, as it was used for some other analysis\n",
    "- the \"number of veterans\" can be also skipped as the main focus lies on the immigrant data.\n",
    " \n",
    "##### Weather data:\n",
    "\n",
    "- The temperature data is provided on a monthly basis\n",
    "- The \"dt\" gives us the the date as a string. We need to transform it to datetime. As it holds data from 1860 on, we only want to keep the rows from 1960 on. \n",
    "- We only want to use the rows where the country is US.\n",
    "- The name of the city is used to map to the weather and then store to weather table in order to get a star schema.\n",
    "\n",
    "##### Possible next steps:\n",
    "Connect airport data based on lat and long. Maybe with some margin.\n",
    "We could also create a helper table to map the ports to cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "The data shall be represented in a star schema. The advantage is, that it is easy to query an easy to understand.\n",
    "\n",
    "// todo add image of star schema\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "In order to pipe the data into the chosen data model, we will use Spark. This has the advantage that we can do the transformations in memory, before writing the data to the tables.\n",
    "1. Load only the needed columns from the immigrant files into a spark dataframe. \n",
    "2. Create the fact_immigration table and write to parquet files.\n",
    "3. Transform the given SAS timestamps and create a dim_date table. And write to parquet file.\n",
    "4. Load the data for the other dimension tables. Write them to parquet files. \n",
    "\n",
    "Whenever possible use a schema when loading, to have a validation of the incoming data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Capstone Project\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_data_part = '/Users/joebsbar/Documents/GitBarbara/data-engineering-nd/capstone-project/data/sas_data/part-00000-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet'\n",
    "\n",
    "# instead of all 28 columns, we only need 10 for the tables we are going to create\n",
    "city_schema = StructType([\n",
    "    Fld(\"cicid\", Short(), False),\n",
    "    Fld(\"arrdate\", Str()),\n",
    "    Fld(\"depdate\", Str()),\n",
    "    Fld(\"i94cit\", Short(), False),\n",
    "    Fld(\"i94res\", Short()),\n",
    "    Fld(\"i94port\", Str()),\n",
    "    Fld(\"fltno\", Str()),\n",
    "    Fld(\"biryear\", Short()),\n",
    "    Fld(\"gender\", Bt()),\n",
    "    Fld(\"i94visa\", Bt())\n",
    "])\n",
    "\n",
    "# read only the needed data for our fact table\n",
    "df_immigration = spark.read.parquet(sas_data_part).select('cicid', 'arrdate', 'depdate', 'i94cit', 'i94res', 'i94port', 'fltno', 'biryear', 'gender', 'i94visa')\n",
    "#df_immigration.count()\n",
    "#df_immigration.printSchema()\n",
    "#df_immigration.head()\n",
    "print(('rows', df_immigration.count(), 'columns', len(df_immigration.columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "\n",
    "# df_immigration.createOrReplaceTempView(\"immigration\")\n",
    "\n",
    "# add timestamp arrival_date\n",
    "#transform and add new column\n",
    "# https://online.stat.psu.edu/stat481/book/export/html/702\n",
    "# https://stackoverflow.com/questions/36412864/convert-numeric-sas-date-to-datetime-in-pandas\n",
    "# only immigrations if us\n",
    "\n",
    "print(df_immigration)\n",
    "print(('rows', df_immigration.count(), 'columns', len(df_immigration.columns)))\n",
    "\n",
    "@udf\n",
    "def sas_to_timestamp(date_sas: float) -> int:\n",
    "    \"\"\"\n",
    "    params: data_sas: a date in sas format\n",
    "    return: a timestamp in seconds\n",
    "    \"\"\"\n",
    "\n",
    "    if date_sas:\n",
    "        datetime = pd.to_timedelta((date_sas), unit='D') + pd.Timestamp('1960-1-1')\n",
    "        timestamp = datetime.timestamp()\n",
    "        return timestamp\n",
    "        \n",
    "df_immigration = df_immigration.withColumn('arrival_ts', sas_to_timestamp(df_immigration['arrdate']))\n",
    "df_immigration = df_immigration.withColumn('departure_ts', sas_to_timestamp(df_immigration['depdate']))\n",
    "df_immigration.show(1)\n",
    "print(('rows', df_immigration.count(), 'columns', len(df_immigration.columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create fact_immigration and write to parquet\n",
    "fact_immigration = df_immigration.select('cicid', 'arrival_ts', 'departure_ts', 'i94cit', 'i94res', 'i94port', 'fltno').dropDuplicates()\n",
    "\n",
    "fact_immigration.head()\n",
    "fact_immigration.write.parquet('./fact_immigration', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# get all existing timestamps and drop duplicats\n",
    "df_time = df_immigration.select('arrival_ts', 'departure_ts')\n",
    "df_time = df_time.select('arrival_ts').unionAll(df_time.select('departure_ts'))\n",
    "df_time = df_time.withColumnRenamed('arrival_ts', 'ts')\n",
    "df_time.dropDuplicates()\n",
    "\n",
    "# create dim_time table\n",
    "dim_time = df_time.select('ts') \\\n",
    "            .withColumn('date', F.from_unixtime(F.col('ts')/1000)) \\\n",
    "            .withColumn('year', F.year('ts')) \\\n",
    "            .withColumn('month', F.month('ts')) \\\n",
    "            .withColumn('week', F.weekofyear('ts')) \\\n",
    "            .withColumn('weekday', F.dayofweek('ts')) \\\n",
    "            .withColumn('day', F.dayofyear('ts')) \\\n",
    "            .withColumn('hour', F.hour('ts'))\n",
    "\n",
    "print(('rows', dim_time.count(), 'columns', len(dim_time.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_time.write.parquet('./dim_time', mode='overwrite', partitionBy=['year', 'month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "weather_schema = StructType([\n",
    "    Fld(\"dt\", Str()),\n",
    "    Fld(\"AverageTemperature\", Str()),\n",
    "    Fld(\"AverageTemperatureUncertainty\", Dbl()),\n",
    "    Fld(\"City\", Int()),\n",
    "    Fld(\"Country\", Int()),\n",
    "    Fld(\"Latitude\", Int()),\n",
    "    Fld(\"Longitude\", Int())\n",
    "])\n",
    "\n",
    "fname = 'data/GlobalLandTemperaturesByCity.csv'\n",
    "df_temperature = spark.read.option(\"delimiter\", \";\").csv(fname, schema=weather_schema, header=True)\n",
    "df_temperature_us = df_temperature[df_temperature[\"Country\"] == \"United States\"]\n",
    "\n",
    "year_format = '%Y-%m-%d'\n",
    "# transform string type to datetype\n",
    "\n",
    "df_temperature_us = df_temperature_us.withColumn(\"date\", to_date(\"dt\"))\n",
    "\n",
    "#df_temperature_us = df_temperature_us.filter(df_temperature_us.date >= \"1970-01-01\")\n",
    "\n",
    "# df_temperature_us.withColumn('date', get_date)\n",
    "# get rid of all rows earlier than 1960\n",
    "# df_temperature_us_data = df_temperature_us_data[df_temperature_us_data[\"date\"] >= '1960-01-01' ]\n",
    "\n",
    "print(df_temperature_us.schema)\n",
    "print(('rows', df_temperature_us.count(), 'columns', len(df_temperature_us.columns)))\n",
    "\n",
    "\n",
    "#display(df_temperature_us)\n",
    "# todo transform datetime to have year, month and dt\n",
    "# todo get rid of country\n",
    "# starts with 1820 -> get rid of all dates previous than \n",
    "# toto get rid of all dates before 1960 - 1820-01-01 \n",
    "#create dim_weather table\n",
    "# store dim_weather table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US City Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'data/us-cities-demographics.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_schema = StructType([\n",
    "    Fld(\"city_name\", Str()),\n",
    "    Fld(\"state\", Str()),\n",
    "    Fld(\"median_age\", Dbl()),\n",
    "    Fld(\"male_population\", Int()),\n",
    "    Fld(\"total_population\", Int()),\n",
    "    Fld(\"foreign_born\", Int()),\n",
    "    Fld(\"average_householdsize\", Int()),\n",
    "    Fld(\"state_code\", Int()),\n",
    "    Fld(\"race\", Str()),\n",
    "])\n",
    "\n",
    "dim_city =  spark.read.option(\"delimiter\", \";\").csv(fname, schema=city_schema, header=True)\n",
    "display(df_city)\n",
    "\n",
    "# or read with header and map\n",
    "# df_city =  spark.read.option(\"header\",True).option(\"delimiter\", \";\").csv(fname)\n",
    "# dim_city = df_city.withColumnRenamed(\"City\",\"city_name\") \\\n",
    "#             .withColumnRenamed(\"State\",\"state\") \\\n",
    "#             .withColumnRenamed(\"Median Age\", \"median_age\") \\\n",
    "#             .withColumnRenamed(\"Male Population\", \"male_population\") \\\n",
    "#             .withColumnRenamed(\"Female Population\", \"female_population\") \\\n",
    "#             .withColumnRenamed(\"Total Population\", \"total_population\") \\\n",
    "#             .withColumnRenamed(\"Foreign-born\", \"foreign_born\") \\\n",
    "#             .withColumnRenamed(\"Average Household Size\", \"average_householdsize\") \\\n",
    "#             .withColumnRenamed(\"State Code\", \"state_code\") \\\n",
    "#             .withColumnRenamed(\"Race\", \"race\") \n",
    "\n",
    "city_code_df = df_immigration[\"i94cit\"]\n",
    "\n",
    "# TODO WHY NOT JOINABLE???\n",
    "dim_city_with_code = dim_city.join(city_code_df)\n",
    "\n",
    "print(dim_city_with_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIM IMMIGRANT PERSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fact_immigration and write to parquet\n",
    "dim_immigrant_person = df_immigration.select('cicid', 'biryear', 'gender', 'i94visa').dropDuplicates()\n",
    "\n",
    "dim_immigrant_person.head()\n",
    "dim_immigrant_person.writeparquet('./dim_immigrant_person', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "The data exploration can be found in `data/exploration.ipynb`\n",
    "\n",
    "#### Cleaning Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Steps - Immigration Data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n",
    "\n",
    "The code for the pipelines can be found in `etl.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
