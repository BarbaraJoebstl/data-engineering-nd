{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for a data warehouse\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This is the capstone project of the udacity nanodegree for data engineering. \n",
    "The aim of the project is to apply learned skills during the course. This project will show how to load and transform data from four different data sources, load the data in spark apply quality checks and store the data into a star schema so that it can be used for BI apps and ad hoc analysis.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* _Step 1_: Scope the Project and Gather Data\n",
    "* _Step 2_: Explore and Assess the Data\n",
    "* _Step 3_: Define the Data Model\n",
    "* _Step 4_: Run ETL to Model the Data\n",
    "* _Step 5_: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "As one of the acceptance criteria for this project is to handle at least 1 million rows and two different data sources and file formats, we will use the data sources are provided by Udacity. \n",
    "The scope of this project is to create a star schema source-of-truth database so that it can be used for BI apps and ad hoc analysis.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "The main dataset includes data about \n",
    "- _Immigration in the United States of America_ from [here](https://www.trade.gov/national-travel-and-tourism-office).\n",
    "\n",
    "Supplementary datasets provided are:\n",
    "\n",
    "- U.S. city _demographics_ from [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    "\n",
    "- _Temperature_ data from [here](https://www.trade.gov/national-travel-and-tourism-office)\n",
    "\n",
    "- Data on _airport codes_ from [here](https://datahub.io/core/airport-codes#data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Explore and Asses the Data\n",
    "The exploration of the used datasets including descriptions of the above mentioned data can be found in  \n",
    "* `exploration.ipynb`. *\n",
    "\n",
    "#### Important findings of the exploration\n",
    "\n",
    "##### Immigrant data:\n",
    " -- dates are provided in SAS timestamps and need to be transformed\n",
    "\n",
    " -- there are many columns that have a lot of null values or are marked with CIC does not use\", hence can be skipped. As the immigrant data is provided in parquet files, we can only read in the columns we need for this project.\n",
    "\n",
    " -- as the data is provided in parquet files, we can only load the data we need\n",
    "\n",
    " -- We might need helper tables to provide data for the\n",
    "\n",
    " --- visa codes: 1 = Business 2 = Pleasure 3 = Student \n",
    " \n",
    " --- travel code:  1 = 'Air' 2 = 'Sea' 3 = 'Land' 9 = 'Not \n",
    "\n",
    "##### City demographics data:\n",
    "- the \"count\" column can be skipped, as it was used for some other analysis\n",
    "- the \"number of veterans\" can be also skipped as the main focus lies on the immigrant data.\n",
    " \n",
    "##### Weather data:\n",
    "\n",
    "- The temperature data is provided on a monthly basis\n",
    "- The \"dt\" gives us the the date as a string. We need to transform it to datetime. As it holds data from 1860 on, we only want to keep the rows from 1960 on. \n",
    "- We only want to use the rows where the country is US.\n",
    "- The name of the city is used to map to the weather and then store to weather table in order to get a star schema.\n",
    "\n",
    "##### Possible next steps:\n",
    "Connect airport data based on lat and long. Maybe with some margin.\n",
    "We could also create a helper table to map the ports to cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "The data shall be used for ad hoc queries and BI Apps. Therefore the data shall be represented in a star schema.\n",
    "The advantage is, that it is easy to query an easy to understand.\n",
    "\n",
    "![star schema](\"./capstone_schema.png\")\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "In order to pipe the data into the chosen data model, we will use Spark. This has the advantage that we can do the transformations in memory, before writing the data to the tables.\n",
    "1. Load only the needed columns from the immigrant files into a spark dataframe. \n",
    "2. Transform arrival and departure date to timestamps\n",
    "3. Create the fact_immigration table from the loaded immigrant files and write to parquet files.\n",
    "4. Create the dim_immigrant_person table from the loaded immigrant files and write to parquet files.\n",
    "5. Create the dim_time table from the transformed timestamps\n",
    "6. Load weather data, filter the weather data by country, as we only need data from the United States from 1960 on and save them as dim_weather\n",
    "7. Load the city data and write to parquet files as dim_city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n",
    "The code can be found in `etl.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import etl_capstone\n",
    "\n",
    "etl_capstone.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks\n",
    "\n",
    "- We are using  _schemas_, when reading the data, with the schema we can assure that the data type is correct. See `schemas.py`\n",
    "- We are using _dropDuplicates()_ to get rid of duplicate data.\n",
    "- We are count the number of rows and columns for the output tables. See `utils.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "#### FACT IMMIGRATION\n",
    "\n",
    "| column name| description | type | data source | \n",
    "| --- | --- | --- | --- |\n",
    "|cicid | unique number for an immigrant | integer, not nullable | sas_data  immigration |\n",
    "|arrival_ts | timestamp  of the arrival date | timestamp | sas_data  immigration: transformed from field \"arrdata\": SAS numeric |\n",
    "|departure_ts | timestamp  of the arrival date | timestamp | sas_data  immigration: transformed from field \"depdate\": SAS numeric |\n",
    "|i94cit | 3 digit code of origin city | short, not nullable | sas_data  immigration |\n",
    "|i94res | 3 digit from the country one has travelled | short | sas_data  immigration |\n",
    "|i94port | 3 char code of origin city in USA| string | sas_data  immigration |\n",
    "|fltno | flight number of airline that arrived in us | string | sas_data  immigration |\n",
    "\n",
    "#### DIM TIME\n",
    "\n",
    "Datasource: all timestamps are taken frome the arrival and departure date of the fact_immigration table\n",
    "\n",
    "| column name| description | type |\n",
    "| --- | --- | --- | \n",
    "| ts | unix timestamp, not nullable | ts |\n",
    "| date | date | string |\n",
    "| year | year | integer |\n",
    "| month | month | integer |\n",
    "| weekday | weekday | integer |\n",
    "| day | day | integer |\n",
    "| hour | hour | integer |\n",
    "\n",
    "#### DIM IMMIGRANT\n",
    "Data Source:  sas_data  immigration\n",
    "\n",
    "| column name| description | type |\n",
    "| --- | --- | --- | \n",
    "|cicid|  unique number for the immigrants| int |\n",
    "|biryear| year of birth| int |\n",
    "|gender| gender of immigrant| string |\n",
    "|i94visa| Visa codes collapsed into three categories, 1 = Business 2 = Pleasure 3 = Student |int |\n",
    "\n",
    "\n",
    "#### DIM CITY\n",
    "\n",
    "Datasource: https://public.opendatasoft.com/explore/dataset/us-cities-demographics\n",
    "\n",
    "| column name| description | type | \n",
    "| --- | --- | --- |\n",
    "|city_name| name of the city| string |\n",
    "|state | state | string |\n",
    "|male_population | number of male population| int |\n",
    "|male_population | number of female population| int |\n",
    "|total_population | number of total population| int |\n",
    "|foreign_born | number of foreign born | int |\n",
    "|average_household | average number of people in a household | double |\n",
    "| state_code | code of the state | int |\n",
    "\n",
    "\n",
    "#### DIM TEMPERATURE\n",
    "\n",
    "| column name| description | type |\n",
    "| --- | --- | --- | \n",
    "|dt|date|date|\n",
    "|AverageTemperature| <- |float|\n",
    "|AverageTemperatureUncertainty|<- |string|\n",
    "|City|<- | string|\n",
    "|Country|<- | string|\n",
    "|year| <- | integer|\n",
    "|month| <- |integer|\n",
    "|Langitude| <- |integer|\n",
    "|Latidue| <- |integer|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "1. _Clearly state the rationale for the choice of tools and technologies for the project._\n",
    "    We choose Apache Spark, because\n",
    "    - we are able to read in only the columns we need from parquet files\n",
    "    - we are able to do data cleaning and transformation either with dataframes or sql syntax.\n",
    "    - it provides an easy to use API\n",
    "    - it can handle a lot of different file formats \n",
    "    - it can handle big amounts of data  <br/>\n",
    "  \n",
    "\n",
    "2. _Propose how often the data should be updated and why._\n",
    "    - It depends on the amount of data and how often the data is updated. If the data is updated every week, it makes sense to run the pipeline every week. <br/>\n",
    "\n",
    "\n",
    "3. _Write a description of how you would approach the problem differently under the following scenarios:_  <br/>\n",
    "\n",
    " 3a. _The data was increased by 100x._<br/>\n",
    "    - The bigger the data gets, the more computing power is helpful to process the data. Adding more nodes to our cluster could be a way of dealing with a bigger amount of data.\n",
    "\n",
    "\n",
    " 3b. _The data populates a dashboard that must be updated on a daily basis by 7am every day._<br/>\n",
    "    - For scheduled pipelines a tool like [Airflow](https://airflow.apache.org/docs/) can be used. It has the advantage, that it provides a web view, so that also non programmers can check wether a pipeline ran successfully or not.\n",
    "\n",
    "\n",
    " 3c. _The database needed to be accessed by 100+ people._<br/>\n",
    "    - There are different cloud solutions availabe for this scenario. We could use [Databricks](https://databricks.com/) it can handle a lot of simultaneasly connections.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
