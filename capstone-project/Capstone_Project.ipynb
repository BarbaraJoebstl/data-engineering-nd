{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for a data warehouse\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The project combines data of the hackernews feed with formula one race dates of a given year\n",
    "\n",
    "The project follows the follow steps:\n",
    "* _Step 1_: Scope the Project and Gather Data\n",
    "\n",
    "* _Step 2_: Explore and Assess the Data\n",
    "\n",
    "* _Step 3_: Define the Data Model\n",
    "\n",
    "* _Step 4_: Run ETL to Model the Data\n",
    "\n",
    "* _Step 5_: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, desc, asc\n",
    "from pyspark.sql.types import StructType, Fieldtype as Fld, StringType as Str, IntegerType as Int, ShortType as Short, DoubleType as Dbl, ByteType as Bt\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "We will use the provided datasets by udacity, to create a source-of-truth database that can be used for queries and as data source for BI apps.\n",
    "Provided datasets: \n",
    "The main dataset includes data about _Immigration in the United States of America_ from [here](https://www.trade.gov/national-travel-and-tourism-office).\n",
    "Supplementary datasets provided are:\n",
    "- U.S. city _demographics_ from [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    "- _Temperature_ data from [here](https://www.trade.gov/national-travel-and-tourism-office)\n",
    "- Data on _airport codes_ from [here](https://datahub.io/core/airport-codes#data)\n",
    "\n",
    "#### Describe and Gather Data \n",
    "The exploration of the used datasets can be found in `data/exploration.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Capstone Project\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_data_part = '/Users/joebsbar/Documents/GitBarbara/data-engineering-nd/capstone-project/data/sas_data/part-00000-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet'\n",
    "\n",
    "# todo how to validat SAS numeric field\n",
    "# instead of all 28 columns, we only need 10 for the tables we are going to create\n",
    "city_schema = StructType([\n",
    "    Fld(\"cicid\", Short(), False),\n",
    "    Fld(\"arrdate\", Str()),\n",
    "    Fld(\"depdate\", Str()),\n",
    "    Fld(\"i94cit\", Short(), False),\n",
    "    Fld(\"i94res\", Short()),\n",
    "    Fld(\"i94port\", Str()),\n",
    "    Fld(\"fltno\", Str()),\n",
    "    Fld(\"biryear\", Short()),\n",
    "    Fld(\"gender\", Bt()),\n",
    "    Fld(\"i94visa\", Bt())\n",
    "])\n",
    "\n",
    "# read only the needed data for our fact table\n",
    "df_immigration = spark.read.parquet(sas_data_part).select('cicid', 'arrdate', 'depdate', 'i94cit', 'i94res', 'i94port', 'fltno', 'biryear', 'gender', 'i94visa')\n",
    "#df_immigration.count()\n",
    "#df_immigration.printSchema()\n",
    "#df_immigration.head()\n",
    "print(('rows', df_immigration.count(), 'columns', len(df_immigration.columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration.describe(\"biryear\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration.dropDuplicates(['cicid']).sort(\"gender\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "\n",
    "# df_immigration.createOrReplaceTempView(\"immigration\")\n",
    "\n",
    "# add timestamp arrival_date\n",
    "#transform and add new column\n",
    "# https://online.stat.psu.edu/stat481/book/export/html/702\n",
    "# https://stackoverflow.com/questions/36412864/convert-numeric-sas-date-to-datetime-in-pandas\n",
    "# only immigrations if us\n",
    "\n",
    "print(df_immigration)\n",
    "print(('rows', df_immigration.count(), 'columns', len(df_immigration.columns)))\n",
    "\n",
    "@udf\n",
    "def sas_to_timestamp(date_sas: float) -> int:\n",
    "    \"\"\"\n",
    "    params: data_sas: a date in sas format\n",
    "    return: a timestamp in seconds\n",
    "    \"\"\"\n",
    "\n",
    "    if date_sas:\n",
    "        datetime = pd.to_timedelta((date_sas), unit='D') + pd.Timestamp('1960-1-1')\n",
    "        timestamp = datetime.timestamp()\n",
    "        return timestamp\n",
    "        \n",
    "df_immigration = df_immigration.withColumn('arrival_ts', sas_to_timestamp(df_immigration['arrdate']))\n",
    "df_immigration = df_immigration.withColumn('departure_ts', sas_to_timestamp(df_immigration['depdate']))\n",
    "df_immigration.show(1)\n",
    "print(('rows', df_immigration.count(), 'columns', len(df_immigration.columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create fact_immigration and write to parquet\n",
    "fact_immigration = df_immigration.select('cicid', 'arrival_ts', 'departure_ts', 'i94cit', 'i94res', 'i94port', 'fltno').dropDuplicates()\n",
    "\n",
    "fact_immigration.head()\n",
    "fact_immigration.writeparquet('./fact_immigration', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# get all existing timestamps and drop duplicats\n",
    "df_time = df_immigration.select('arrival_ts', 'departure_ts')\n",
    "df_time = df_time.select('arrival_ts').unionAll(df_time.select('departure_ts'))\n",
    "df_time = df_time.withColumnRenamed('arrival_ts', 'ts')\n",
    "df_time.dropDuplicates()\n",
    "\n",
    "# create dim_time table\n",
    "dim_time = df_time.select('ts') \\\n",
    "            .withColumn('date', F.from_unixtime(F.col('ts')/1000)) \\\n",
    "            .withColumn('year', F.year('ts')) \\\n",
    "            .withColumn('month', F.month('ts')) \\\n",
    "            .withColumn('week', F.weekofyear('ts')) \\\n",
    "            .withColumn('weekday', F.dayofweek('ts')) \\\n",
    "            .withColumn('day', F.dayofyear('ts')) \\\n",
    "            .withColumn('hour', F.hour('ts'))\n",
    "\n",
    "print(('rows', dim_time.count(), 'columns', len(dim_time.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_time.write.parquet('./dim_time', mode='overwrite', partitionBy=['year', 'month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weather_schema = StructType([\n",
    "    Fld(\"dt\", Str()),\n",
    "    Fld(\"AverageTemperature\", Str()),\n",
    "    Fld(\"AverageTemperatureUncertainty\", Dbl()),\n",
    "    Fld(\"City\", Int()),\n",
    "    Fld(\"Country\", Int()),\n",
    "    Fld(\"Latitude\", Int()),\n",
    "    Fld(\"Longitude\", Int())\n",
    "])\n",
    "\n",
    "fname = 'data/GlobalLandTemperaturesByCity.csv'\n",
    "df_temperature = spark.read.option(\"delimiter\", \";\").csv(fname, schema=weather_schema, header=True)\n",
    "df_temperature.head()\n",
    "\n",
    "# todo transform datetime to have year, month and dt\n",
    "\n",
    "#create dim_weather table\n",
    "# store dim_weather table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US City Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'data/us-cities-demographics.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_schema = StructType([\n",
    "    Fld(\"city_name\", Str()),\n",
    "    Fld(\"state\", Str()),\n",
    "    Fld(\"median_age\", Dbl()),\n",
    "    Fld(\"male_population\", Int()),\n",
    "    Fld(\"total_population\", Int()),\n",
    "    Fld(\"foreign_born\", Int()),\n",
    "    Fld(\"average_householdsize\", Int()),\n",
    "    Fld(\"state_code\", Int()),\n",
    "    Fld(\"race\", Str()),\n",
    "])\n",
    "\n",
    "dim_city =  spark.read.option(\"delimiter\", \";\").csv(fname, schema=city_schema, header=True)\n",
    "display(df_city)\n",
    "\n",
    "# or read with header and map\n",
    "# df_city =  spark.read.option(\"header\",True).option(\"delimiter\", \";\").csv(fname)\n",
    "# dim_city = df_city.withColumnRenamed(\"City\",\"city_name\") \\\n",
    "#             .withColumnRenamed(\"State\",\"state\") \\\n",
    "#             .withColumnRenamed(\"Median Age\", \"median_age\") \\\n",
    "#             .withColumnRenamed(\"Male Population\", \"male_population\") \\\n",
    "#             .withColumnRenamed(\"Female Population\", \"female_population\") \\\n",
    "#             .withColumnRenamed(\"Total Population\", \"total_population\") \\\n",
    "#             .withColumnRenamed(\"Foreign-born\", \"foreign_born\") \\\n",
    "#             .withColumnRenamed(\"Average Household Size\", \"average_householdsize\") \\\n",
    "#             .withColumnRenamed(\"State Code\", \"state_code\") \\\n",
    "#             .withColumnRenamed(\"Race\", \"race\") \n",
    "\n",
    "city_code_df = df_immigration[\"i94cit\"]\n",
    "\n",
    "# TODO WHY NOT JOINABLE???\n",
    "dim_city_with_code = dim_city.join(city_code_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIM IMMIGRANT PERSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fact_immigration and write to parquet\n",
    "dim_immigrant_person = df_immigration.select('cicid', 'biryear', 'gender', 'i94visa').dropDuplicates()\n",
    "\n",
    "dim_immigrant_person.head()\n",
    "dim_immigrant_person.writeparquet('./dim_immigrant_person', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "The data exploration can be found in `data/exploration.ipynb`\n",
    "\n",
    "#### Cleaning Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Steps - Immigration Data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
